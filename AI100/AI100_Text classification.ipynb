{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI100 Text classification introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are comparisions of different strategies for text classification task on [AI100](http://competition.ai100.com.cn/html/game_det.html?id=24&tab=1). [The third place](http://geek.ai100.com.cn/2017/06/01/1665) in the end.\n",
    "\n",
    "The following chapters include three parts: part 1 (bag of words), part 2 (LDA) and part 3 (word2vec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try statistical machine learning strategy, i.e., [bag of words (bow)](https://en.wikipedia.org/wiki/Bag-of-words_model) + different algorithms. \n",
    "\n",
    "For unstructured text data, we should extract numerical features vectors first. \n",
    "\n",
    "Here we use bag of words to represent text. The training data (corpus) are converted to a matrix, which each row represent document and columns are word vocabulary appeared in the whole corpus. And the element of the matrix can be word counts or tf-idf. Please refer to [here](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) for more details.\n",
    "\n",
    "Note that the above matrix is **high dimensional sparse** !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: reading input and text segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will import necessary packages first (such as [scikit-learn](http://scikit-learn.org/), [pandas](http://pandas.pydata.org/), [numpy](http://www.numpy.org/), [jieba](https://github.com/fxsjy/jieba)), load input data and perform Chinese text segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba as jb\n",
    "import codecs\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = 'materials' # input file directory\n",
    "# read input\n",
    "training_data = pd.read_csv(os.path.join(data_dir, 'training.csv'), names=['text_label', 'text_content'], encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# size of training data\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take a glance \n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the number of each category\n",
    "# we see that the training data is unbalanced\n",
    "pd.value_counts(training_data['text_label'].values)#.plot(kind='bar')\n",
    "# or training_data.groupby(training_data['text_label']).count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function: reading file\n",
    "def read_file(file_path):\n",
    "    f = codecs.open(file_path, encoding='utf-8')\n",
    "    lines = []\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n').rstrip('\\r')\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "stopwordsCN = read_file(os.path.join(data_dir, 'stopWords_cn.txt'))\n",
    "\n",
    "# helper function: text segmentation\n",
    "def cut_content(each_row):\n",
    "    return ' '.join([word for word in jb.lcut(each_row['text_content']) if word not in stopwordsCN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at the first ten rows results\n",
    "training_data.head(10)['text_content_segmentation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# manually add specific words\n",
    "new_words = read_file(os.path.join(data_dir, 'ai100_words.txt'))\n",
    "for word in new_words:\n",
    "    jb.add_word(word)\n",
    "    \n",
    "# perform text segmentation\n",
    "training_data['text_content_segmentation'] = training_data.apply(cut_content, axis=1)\n",
    "#testing_data.head(10)\n",
    "#selected_index = range(1001, 1200)\n",
    "#print selected_index\n",
    "#training_data.iloc[selected_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will extract tf-idf numerical features using [CounterVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word count feature\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_data['text_content_segmentation'])\n",
    "# play with X_train_counts: https://de.dariah.eu/tatom/working_with_text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect.vocabulary_#.get(u'企业')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf-idf feature\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: build classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use linear [Support Vector Machine (SVM)](https://github.com/jakevdp/sklearn_pycon2015/blob/master/notebooks/03.1-Classification-SVMs.ipynb) method to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linear SVM\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "svm_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 10-fold cross-validation results\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "cv_scores = cross_val_score(svm_clf, X_train_tfidf, training_data['text_label'], cv=KFold(n_splits=10), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we build a [pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) for feature extracting and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('word_counter', CountVectorizer()),\n",
    "                    ('tfidf_computer', TfidfTransformer()),\n",
    "                    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf = text_clf.fit(training_data['text_content_segmentation'], training_data['text_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(text_clf, training_data['text_content_segmentation'], training_data['text_label'], cv=KFold(n_splits=10), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the above model is used to preform predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the test data\n",
    "testing_data = pd.read_csv(os.path.join(data_dir, 'testing.csv'), names=['text_index', 'text_content'], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_data['text_content_segmentation'] = testing_data.apply(cut_content, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testing_data.head(10)\n",
    "selected_index = range(81, 90)\n",
    "print selected_index\n",
    "testing_data.iloc[selected_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_res = text_clf.predict(testing_data['text_content_segmentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to csv file\n",
    "np.savetxt(os.path.join(data_dir, 'results.csv'), np.dstack((np.arange(1, predict_res.size+1), predict_res))[0],\"%d,%d\")\n",
    "# submit accuracy: 0.836"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 6: debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Several directions can be done...\n",
    "\n",
    "[1] model parameter tuning, riched feature (n-gram or word2vec)\n",
    "\n",
    "[2] feature selection\n",
    "\n",
    "[3] try XGBoost and liblinear\n",
    "\n",
    "[4] model ensemble\n",
    "\n",
    "[5] deal with imbalanced dataset\n",
    "\n",
    "[6] better text segmentation vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following codes shows the solution of [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('word_counter', CountVectorizer()),\n",
    "                    ('tfidf_computer', TfidfTransformer()),\n",
    "                    ('clf', SGDClassifier(loss='hinge'))])\n",
    "parameters = {\n",
    "    'word_counter__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'word_counter__min_df': (1, 2, 3, 4, 5),\n",
    "    \n",
    "    'tfidf_computer__norm': ('l1', 'l2'),\n",
    "    'tfidf_computer__use_idf': (True, False),\n",
    "    'tfidf_computer__smooth_idf': (True, False),\n",
    "    'tfidf_computer__sublinear_tf': (True, False)\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(training_data['text_content_segmentation'], training_data['text_label'])\n",
    "\n",
    "print gs_clf.best_score_\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "# submit accuracy: 0.858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We try [2] feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "text_clf = Pipeline([('word_counter', CountVectorizer()),\n",
    "                    ('tfidf_computer', TfidfTransformer()),\n",
    "                    ('select_feature', SelectKBest(chi2)),\n",
    "                    ('clf', SGDClassifier(loss='hinge', penalty='elasticnet', class_weight='balanced', alpha=1e-05, n_iter=20))])\n",
    "\n",
    "parameters = {\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "    #'word_counter__ngram_range': [(1, 1), (1, 2)],\n",
    "    #'word_counter__min_df': (1, 2, 3, 4, 5),\n",
    "\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "    #'tfidf_computer__smooth_idf': (True, False),\n",
    "    #'tfidf_computer__sublinear_tf': (True, False),\n",
    "\n",
    "    #'select_feature__score_func': (),\n",
    "    'select_feature__k': (10000, 20000, 25000) #50000, 100000, 150000\n",
    "\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "    #'clf__penalty': ('l1', 'l2', 'elasticnet'),\n",
    "    #'clf__alpha': (0.001, 0.0001, 0.00001),\n",
    "    #'clf__n_iter': (20, 30, 50, 100, 200),\n",
    "    #'clf__class_weight': ('balanced', None)\n",
    "    #'clf__C': (1, 10, 100, 1000),\n",
    "    #'clf__penalty': ('l1', 'l2'),\n",
    "    #'clf__loss': ('hinge', 'squared_hinge')\n",
    "    #'clf__multi_class': ('ovr', 'crammer_singer'),\n",
    "    #'clf__max_iter': (1000, 2000)\n",
    "}\n",
    "\n",
    "print 'start to grid search'\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1, cv=10)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(training_data['text_content_segmentation'], training_data['text_label'])\n",
    "\n",
    "gs_clf.fit(training_data['text_content_segmentation'], training_data['text_label'])\n",
    "#gs_clf.fit(X_train, y_train)\n",
    "\n",
    "print 'best results:'\n",
    "print gs_clf.best_score_\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then [3] try XGBoost and liblinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[XGBoost](http://xgboost.readthedocs.io/en/latest/) is a very popular package in [Kaggle](https://www.kaggle.com/) competiton. Here, we have a look at the power of XGBoost. However, xgboost seems not work for high dimensional and sparse features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# references: \n",
    "# http://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "# https://jessesw.com/XG-Boost/\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_data['text_content_segmentation'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "\n",
    "cv_params = {\n",
    "    'max_depth': [3, 4],\n",
    "    'min_child_weight': [1, 3]\n",
    "}\n",
    "\n",
    "init_params = {'learning_rate': 0.1, 'n_estimators': 1000, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, 'objective': 'multi:softmax'}\n",
    "\n",
    "print 'start to gs:'\n",
    "gs_clf = GridSearchCV(xgb.XGBClassifier(**init_params), cv_params, scoring='accuracy', n_jobs=-1)\n",
    "gs_clf.fit(X_train_tfidf.todense(), training_data['text_label'])\n",
    "\n",
    "print 'best results:'\n",
    "print gs_clf.best_score_\n",
    "\n",
    "for param_name in sorted(cv_params.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we do [4] model ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# references: \n",
    "# http://sebastianraschka.com/Articles/2014_ensemble_classifier.html\n",
    "# https://stats.stackexchange.com/questions/190151/ensembling-with-votingclassifier\n",
    "# http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "estimators = []\n",
    "svm_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, n_iter=5)\n",
    "estimators.append(('svm', svm_clf))\n",
    "\n",
    "mnb = MultinomialNB(alpha=0.1)\n",
    "estimators.append(('nb', mnb))\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=50)\n",
    "estimators.append(('rf', rf_clf))\n",
    "\n",
    "voting = VotingClassifier(estimators)\n",
    "\n",
    "text_clf = Pipeline([('word_counter', CountVectorizer(ngram_range=(1, 2))),\n",
    "                     ('tf_idf_computer', TfidfTransformer()),\n",
    "                     ('clf', voting)])\n",
    "\n",
    "cv_scores = cross_val_score(text_clf, training_data['text_content_segmentation'], training_data['text_label'], cv=KFold(n_splits=3), n_jobs=-1)\n",
    "ss = text_clf.fit(training_data['text_content_segmentation'], training_data['text_label'])\n",
    "\n",
    "print cv_scores\n",
    "print np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we [5] deal with imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# references: \n",
    "# http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "# http://blog.csdn.net/heyongluoyao8/article/details/49408131\n",
    "# http://www.jianshu.com/p/3e8b9f2764c8\n",
    "# https://github.com/ThoughtWorksInc/dataclouds/blob/master/source/_posts/%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86.md\n",
    "\n",
    "'''\n",
    "# subsampling\n",
    "sample_size = 200\n",
    "\n",
    "def sample_data(each_group, sam_size):\n",
    "    if len(each_group.index) > sam_size:\n",
    "        return each_group.loc[np.random.choice(each_group.index, sam_size, False),:]\n",
    "    else:\n",
    "        return each_group\n",
    "    \n",
    "banlanced_data = training_data.groupby(training_data['text_label']).apply(lambda x: sample_data(x, sample_size))\n",
    "\n",
    "pd.value_counts(banlanced_data['text_label'].values)\n",
    "text_clf = text_clf.fit(banlanced_data['text_content_segmentation'], banlanced_data['text_label'])\n",
    "cv_scores = cross_val_score(text_clf, banlanced_data['text_content_segmentation'], banlanced_data['text_label'], cv=KFold(n_splits=10), n_jobs=-1)\n",
    "np.mean(cv_scores)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "count_vect = CountVectorizer(min_df=1)\n",
    "X_train_counts = count_vect.fit_transform(training_data['text_content_segmentation'])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "print 'RandomOverSampler'\n",
    "#sm = SMOTEENN()\n",
    "#X_resampled, y_resampled = sm.fit_sample(X_train_tfidf.todense(), training_data['text_label'])\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_sample(X_train_tfidf.todense(), training_data['text_label'])\n",
    "print('Resampled dataset shape {}'.format(Counter(y_resampled)))\n",
    "#Resampled dataset shape Counter({1: 1271, 2: 1271, 3: 1271, 4: 1271, 5: 1271, 6: 1271, 7: 1271, 8: 1271, 9: 1271, 10: 1271, 11: 1271})\n",
    "\n",
    "print 'svm'\n",
    "text_clf = SGDClassifier(loss='hinge', penalty='l2', class_weight='balanced', n_iter=20)\n",
    "text_clf.fit(X_resampled, y_resampled)\n",
    "\n",
    "print 'predict'\n",
    "testing_data = pd.read_csv(os.path.join(data_dir, 'testing.csv'), names=['text_index', 'text_content'])\n",
    "testing_data['text_content_segmentation'] = testing_data.apply(cut_content, axis=1)\n",
    "\n",
    "X_new_counts = count_vect.transform(testing_data['text_content_segmentation'])\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "\n",
    "tested_predicted = text_clf.predict(X_new_tfidf.todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to do...\n",
    "# references: \n",
    "# https://gist.github.com/aronwc/8248457\n",
    "# http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/\n",
    "# https://github.com/chrisjmccormick/LSA_Classification/blob/master/runClassification_LSA.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Word2vec](https://en.wikipedia.org/wiki/Word2vec) is a group of related models (continuous bag-of-words (CBOW) and skip-gram) that are used to produce word embeddings, which is a distributed representation of words. \n",
    "\n",
    "Here, we use [gensim](https://radimrehurek.com/gensim/) package to implement Word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Word2vec + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# references: \n",
    "# http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "%load materials/word2vec/word2vec+svm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Solution 2: Doc2vec + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# references: \n",
    "#http://www.shuang0420.com/2016/06/01/gensim-doc2vec%E5%AE%9E%E6%88%98/\n",
    "#https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb\n",
    "#http://stackoverflow.com/questions/31321209/doc2vec-how-to-get-document-vectors\n",
    "#https://datascience.stackexchange.com/questions/10216/doc2vec-how-to-label-the-paragraphs-gensim\n",
    "#https://groups.google.com/forum/#!topic/word2vec-toolkit/HpVnMfeo5PM\n",
    "#https://github.com/linanqiu/word2vec-sentiments/blob/master/word2vec-sentiment.ipynb\n",
    "#http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "%load materials/word2vec/doc2vec+svm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution 3: FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe due to the lack of more training data, the accuracy of FastText is about 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load materials/Fasttext/AI100_step1_prepare_training_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load materials/Fasttext/AI100_step2_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load materials/Fasttext/AI100_step3_predicting.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: Word2vec + CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
