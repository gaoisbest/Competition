{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do pages:\n",
    "http://stefansavev.com/blog/text-classification-with-scikit-learn/\n",
    "http://fastml.com/classifying-text-with-bag-of-words-a-tutorial/\n",
    "http://www.sohu.com/a/130642494_355135?_f=v2-index-feeds\n",
    "http://www.cnblogs.com/DjangoBlog/p/6202507.html\n",
    "http://albertxiebnu.github.io/fasttext/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI100 Text classification introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are comparisions of different strategies for text classification task on [AI100](http://competition.ai100.com.cn/html/game_det.html?id=24&tab=1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we try traditional machine learning strategy, i.e., [bag of words (bow)](https://en.wikipedia.org/wiki/Bag-of-words_model) + different algorithms. \n",
    "\n",
    "For unstructured text data, we should extract numerical features vectors first. \n",
    "\n",
    "Here we use bag of words to represent text. The training data (corpus) are converted to a matrix, which each row represent document and columns are word vocabulary appeared in the whole corpus. And the element of the matrix can be word counts or tf-idf. Please refer to [here](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) for more details.\n",
    "\n",
    "Note that the above matrix is high dimensional sparse !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1: reading input and text segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will import necessary packages first (such as [scikit-learn](http://scikit-learn.org/), [pandas](http://pandas.pydata.org/), [numpy](http://www.numpy.org/), [jieba](https://github.com/fxsjy/jieba)), load input data and perform Chinese text segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba as jb\n",
    "import codecs\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = 'materials' # input file directory\n",
    "# read input\n",
    "training_data = pd.read_csv(os.path.join(data_dir, 'training.csv'), names=['text_label', 'text_content'], encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4774, 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of training data\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take a glance \n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute the number of each category\n",
    "pd.value_counts(training_data['text_label'].values)#.plot(kind='bar')\n",
    "# or training_data.groupby(training_data['text_label']).count().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function: reading file\n",
    "def read_file(file_path):\n",
    "    f = codecs.open(file_path, encoding='utf-8')\n",
    "    lines = []\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n').rstrip('\\r')\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "stopwordsCN = read_file(os.path.join(data_dir, 'stopWords_cn.txt'))\n",
    "\n",
    "# helper function: text segmentation\n",
    "def cut_content(each_row):\n",
    "    return ' '.join([word for word in jb.lcut(each_row['text_content']) if word not in stopwordsCN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perform text segmentation\n",
    "training_data['text_content_segmentation'] = training_data.apply(cut_content, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at the first ten rows results\n",
    "training_data.head(10)['text_content_segmentation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_words = read_file(os.path.join(data_dir, 'ai100_words.txt'))\n",
    "for word in new_words:\n",
    "    jb.add_word(word)\n",
    "\n",
    "training_data['text_content_segmentation'] = training_data.apply(cut_content, axis=1)\n",
    "#testing_data.head(10)\n",
    "selected_index = range(1001, 1200)\n",
    "print selected_index\n",
    "training_data.iloc[selected_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "WindowsError",
     "evalue": "[Error 126] The specified module could not be found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWindowsError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-7c5e8e2bb436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtgrocery\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGrocery\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;31m# Create a grocery(don't forget to set a name)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrocery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGrocery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sample'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_src\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\RECHAOS-012\\Anaconda_2.7\\lib\\site-packages\\tgrocery\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconverter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Grocery'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\RECHAOS-012\\Anaconda_2.7\\lib\\site-packages\\tgrocery\\classifier.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconverter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGroceryTextConverter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlearner\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\RECHAOS-012\\Anaconda_2.7\\lib\\site-packages\\tgrocery\\learner\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlearner\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mlearner\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\RECHAOS-012\\Anaconda_2.7\\lib\\site-packages\\tgrocery\\learner\\learner.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mizip\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mutil\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCDLL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'util.so.1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mLIBLINEAR_HOME\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LIBLINEAR_HOME'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/liblinear'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\RECHAOS-012\\Anaconda_2.7\\lib\\ctypes\\__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWindowsError\u001b[0m: [Error 126] The specified module could not be found"
     ]
    }
   ],
   "source": [
    "from tgrocery import Grocery\n",
    "# Create a grocery(don't forget to set a name)\n",
    "grocery = Grocery('sample')\n",
    "train_src = []\n",
    "for index, row in training_data.iterrows():\n",
    "    train_src.append((row['text_label'], row['text_content_segmentation']))\n",
    "    \n",
    "print 'len of train_src:{}'.format(len(train_src))\n",
    "\n",
    "\n",
    "grocery.train(train_src)\n",
    "\n",
    "\n",
    "def grocery_predict(each_row):\n",
    "    return grocery.predict(each_row['text_content_segmentation'])\n",
    "\n",
    "print 'predict'\n",
    "training_data['predict_label'] = training_data.apply(grocery_predict, axis=1)\n",
    "\n",
    "print 'result:'\n",
    "print np.mean(training_data['predict_label'] == training_data['text_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: extracting features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will extract tf-idf numerical features using [CounterVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [TfidfTransformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word count feature\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training_data['text_content_segmentation'])\n",
    "# play with X_train_counts: https://de.dariah.eu/tatom/working_with_text.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vect.vocabulary_#.get(u'企业')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf-idf feature\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: build classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use linear [Support Vector Machine (SVM)](https://github.com/jakevdp/sklearn_pycon2015/blob/master/notebooks/03.1-Classification-SVMs.ipynb) method to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linear SVM\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "svm_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 10-fold cross-validation results\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "cv_scores = cross_val_score(svm_clf, X_train_tfidf, training_data['text_label'], cv=KFold(n_splits=10), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we build a [pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) for feature extracting and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('word_counter', CountVectorizer()),\n",
    "                    ('tfidf_computer', TfidfTransformer()),\n",
    "                    ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf = text_clf.fit(training_data['text_content_segmentation'], training_data['text_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(text_clf, training_data['text_content_segmentation'], training_data['text_label'], cv=KFold(n_splits=10), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the above model is used to preform predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_words = read_file(os.path.join(data_dir, 'ai100_words.txt'))\n",
    "for word in new_words:\n",
    "    jb.add_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the test data\n",
    "testing_data = pd.read_csv(os.path.join(data_dir, 'testing.csv'), names=['text_index', 'text_content'], encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_data['text_content_segmentation'] = testing_data.apply(cut_content, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testing_data.head(10)\n",
    "selected_index = range(81, 90)\n",
    "print selected_index\n",
    "testing_data.iloc[selected_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_res = text_clf.predict(testing_data['text_content_segmentation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predict_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to csv file\n",
    "np.savetxt(os.path.join(data_dir, 'results.csv'), np.dstack((np.arange(1, predict_res.size+1), predict_res))[0],\"%d,%d\")\n",
    "# submit accuracy: 0.836"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 6: debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Several directions can be done...\n",
    "\n",
    "[1] model parameter tuning, riched feature (n-gram or word2vec)\n",
    "\n",
    "[2] feature selection\n",
    "\n",
    "[3] try XGBoost and liblinear\n",
    "\n",
    "[4] model ensemble\n",
    "\n",
    "[5] deal with imbalanced dataset\n",
    "\n",
    "[6] better text segmentation vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following codes shows the solution of [1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('word_counter', CountVectorizer()),\n",
    "                    ('tfidf_computer', TfidfTransformer()),\n",
    "                    ('clf', SGDClassifier(loss='hinge'))])\n",
    "parameters = {\n",
    "    'word_counter__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'word_counter__min_df': (1, 2, 3, 4, 5),\n",
    "    \n",
    "    'tfidf_computer__norm': ('l1', 'l2'),\n",
    "    'tfidf_computer__use_idf': (True, False),\n",
    "    'tfidf_computer__smooth_idf': (True, False),\n",
    "    'tfidf_computer__sublinear_tf': (True, False)\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(training_data['text_content_segmentation'], training_data['text_label'])\n",
    "\n",
    "print gs_clf.best_score_\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "# submit accuracy: 0.858"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We try [2] feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then [3] try XGBoost and liblinear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[XGBoost](http://xgboost.readthedocs.io/en/latest/) is a very popular package in [Kaggle](https://www.kaggle.com/) competiton. Here, we have a look at the power of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# references: \n",
    "# http://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\n",
    "# https://jessesw.com/XG-Boost/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we do [4] model ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://sebastianraschka.com/Articles/2014_ensemble_classifier.html\n",
    "#https://stats.stackexchange.com/questions/190151/ensembling-with-votingclassifier\n",
    "#http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/\n",
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "\n",
    "'''\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "estimators = []\n",
    "svm_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, n_iter=5)\n",
    "estimators.append(('svm', svm_clf))\n",
    "\n",
    "mnb = MultinomialNB(alpha=0.1)\n",
    "estimators.append(('nb', mnb))\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=50)\n",
    "estimators.append(('rf', rf_clf))\n",
    "\n",
    "voting = VotingClassifier(estimators)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we [5] deal with imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "# http://blog.csdn.net/heyongluoyao8/article/details/49408131\n",
    "# http://www.jianshu.com/p/3e8b9f2764c8\n",
    "# https://github.com/ThoughtWorksInc/dataclouds/blob/master/source/_posts/%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86.md\n",
    "    \n",
    "sample_size = 200\n",
    "\n",
    "def sample_data(each_group, sam_size):\n",
    "    if len(each_group.index) > sam_size:\n",
    "        return each_group.loc[np.random.choice(each_group.index, sam_size, False),:]\n",
    "    else:\n",
    "        return each_group\n",
    "    \n",
    "banlanced_data = training_data.groupby(training_data['text_label']).apply(lambda x: sample_data(x, sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd.value_counts(training_data['text_label'].values)\n",
    "pd.value_counts(banlanced_data['text_label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf = text_clf.fit(banlanced_data['text_content_segmentation'], banlanced_data['text_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(text_clf, banlanced_data['text_content_segmentation'], banlanced_data['text_label'], cv=KFold(n_splits=10), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://gist.github.com/aronwc/8248457\n",
    "http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/\n",
    "https://github.com/chrisjmccormick/LSA_Classification/blob/master/runClassification_LSA.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Word2vec](https://en.wikipedia.org/wiki/Word2vec) is a group of related models (continuous bag-of-words (CBOW) and skip-gram) that are used to produce word embeddings, which is a distributed representation of words. \n",
    "\n",
    "Question: 'man' - 'woman' = 'king' - ?\n",
    "\n",
    "Here, we use [gensim](https://radimrehurek.com/gensim/) package to implement Word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1: Word2vec + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n",
    "%load materials/word2vec/word2vec+svm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Solution 2: Doc2vec + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://www.shuang0420.com/2016/06/01/gensim-doc2vec%E5%AE%9E%E6%88%98/\n",
    "#https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb\n",
    "#http://stackoverflow.com/questions/31321209/doc2vec-how-to-get-document-vectors\n",
    "#https://datascience.stackexchange.com/questions/10216/doc2vec-how-to-label-the-paragraphs-gensim\n",
    "#https://groups.google.com/forum/#!topic/word2vec-toolkit/HpVnMfeo5PM\n",
    "#https://github.com/linanqiu/word2vec-sentiments/blob/master/word2vec-sentiment.ipynb\n",
    "#http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Solution 3: FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe due to the lack of more training data, the accuracy of FastText is about 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load materials/Fasttext/AI100_step1_prepare_training_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load materials/Fasttext/AI100_step2_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load materials/Fasttext/AI100_step3_predicting.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 4: Word2vec + CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
